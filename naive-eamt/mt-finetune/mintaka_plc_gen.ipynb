{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005ebcb-ad0e-43dd-b158-e30f30a4947f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the mintaka dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af9b695-fac9-4de3-aaed-14397b1df53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from string import Template\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f0e2a6-616c-4d45-9fcf-368856f27bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 5630/14000 [1:11:22<1:46:07,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../eval/dataset/mintaka/mintaka_train.json') as mintaka_train_file:\n",
    "    mintaka_train_json = json.load(mintaka_train_file)\n",
    "pbar = tqdm(total=len(mintaka_train_json), position=0, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89fa8091-1d7d-49e6-aa71-a5e28536cfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mintaka_train_json[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbb8651c-3fe5-42b6-8bfb-f1ab65b21098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accepted_langs = ['de', 'pt', 'es', 'fr']\n",
    "accepted_langs = ['it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a5a7a9-2297-463f-87a4-f0fa2def6a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safeget(dct, *keys):\n",
    "    for key in keys:\n",
    "        try:\n",
    "            dct = dct[key]\n",
    "        except KeyError:\n",
    "            return None\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9de916c7-65fc-4e13-b663-a482e6ce3d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparql_wd = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql_wd.setReturnFormat(JSON)\n",
    "\n",
    "# Temporary agent modifier\n",
    "agent_header = {'User-Agent': 'wiki_parser_online/0.17.1 (https://deeppavlov.ai;'\n",
    "                              ' info@deeppavlov.ai) deeppavlov/0.17.1'}\n",
    "sparql_wd.agent = str(agent_header)\n",
    "\n",
    "langmatches_clause = \" langMatches( lang(?enlbl), \\\"en\\\") \"\n",
    "for lang in accepted_langs:\n",
    "    langmatches_clause += \" || langMatches( lang(?lbl), \\\"\" + lang + \"\\\") \"\n",
    "\n",
    "WD_QUERY_STR = Template('''\n",
    "    SELECT ?lbl (lang(?lbl) as ?lang) WHERE {\n",
    "        OPTIONAL {\n",
    "            wd:$link rdfs:label ?lbl .\n",
    "            FILTER ( %s )\n",
    "        }\n",
    "    }\n",
    "''' % langmatches_clause)\n",
    "\n",
    "\n",
    "def find_entity_labels(entity_id):\n",
    "    ret_dict = {}\n",
    "    f_sparql = WD_QUERY_STR.substitute(link=entity_id)\n",
    "    # print('SPARQL: %s' % f_sparql)\n",
    "    sparql_wd.setQuery(f_sparql)\n",
    "    ret = sparql_wd.queryAndConvert()\n",
    "    # print('SPARQL results: %s' % ret)\n",
    "    for r in ret[\"results\"][\"bindings\"]:\n",
    "        cur_lang = safeget(r, 'lang', 'value')\n",
    "        cur_lbl = safeget(r, 'lbl', 'value')\n",
    "        # print('Current label: %s\\nCurrent language: %s\\n\\n' % (cur_lbl, cur_lang))\n",
    "        if cur_lang and cur_lbl and (cur_lang in accepted_langs):\n",
    "            ret_dict[cur_lang] = cur_lbl\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dca3509-3ca6-461f-a676-3d56428abe04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_placeholder_str(query, ent_links):\n",
    "    query_plc = ''\n",
    "    last_ind = 0\n",
    "    for link in ent_links:\n",
    "        if 'link' not in link:\n",
    "            continue\n",
    "        plchldr = link['placeholder']\n",
    "        # forming the placeholder query\n",
    "        if plchldr:\n",
    "            query_plc += query[last_ind:link['start']] + plchldr\n",
    "            last_ind = link['end']\n",
    "    query_plc += query[last_ind:]\n",
    "    return query_plc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "240b25b3-2873-46a8-8bfe-5856a20c648d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each entry in mintaka dataset\n",
    "    # For each entity in the entry\n",
    "        # Extract other language labels\n",
    "        # For each entry check if label exists in the query of that language\n",
    "    # For each language that has matching number of entities with original English text\n",
    "        # Write the entry to reference and prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64bbc9ba-37f7-4f32-8253-74c30852b924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_ent_mentions(entity_info_array, plc):\n",
    "    ent_mentions = []\n",
    "    arr_ind = 1\n",
    "    entity_info_array.sort(key=lambda d: d['span'][0])\n",
    "    for info in entity_info_array:\n",
    "        enttype = info.get('entityType')\n",
    "        if enttype == 'entity':\n",
    "            mention = {'start': info['span'][0], 'end': info['span'][1],\n",
    "                       'surfaceform': info['mention'], 'link': info['name'],\n",
    "                       'placeholder': '[%s%d]' % (plc, arr_ind)}\n",
    "            ent_mentions.append(mention)\n",
    "            arr_ind += 1\n",
    "    # ent_mentions.sort(key=lambda d: d['start'])\n",
    "    return ent_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6ed680-5335-4ceb-9fce-41eecf808d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mintaka_train_json = mintaka_train_json[0:3]\n",
    "# print(len(mintaka_train_json))\n",
    "# print(mintaka_train_json[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb223ba1-2ad4-4588-9e88-4a5aebdc5f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the output files\n",
    "output_dir = 'data/'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "op_files = {}\n",
    "for key in accepted_langs:\n",
    "    plc_en_file = '%s%s_en_placeholder.txt' % (output_dir, key)\n",
    "    plc_lang_file = '%s%s_placeholder.txt' % (output_dir, key)\n",
    "    norm_en_file = '%s%s_en.txt' % (output_dir, key)\n",
    "    norm_lang_file = '%s%s.txt' % (output_dir, key)\n",
    "    en_file_obj = open(plc_en_file, 'w')\n",
    "    lang_file_obj = open(plc_lang_file, 'w')\n",
    "    norm_en_file_obj = open(norm_en_file, 'w')\n",
    "    norm_lang_file_obj = open(norm_lang_file, 'w')\n",
    "    op_files[key] = (en_file_obj, lang_file_obj, norm_en_file_obj, norm_lang_file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "695e613f-096b-4550-9527-5b377b9fafcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14000/14000 [1:02:40<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "for entry in mintaka_train_json:\n",
    "    eng_text = entry['question']\n",
    "    ent_mentions = extract_ent_mentions(entry['questionEntity'], '00')\n",
    "    # print('Entity mentions: %s' % ent_mentions)\n",
    "    eng_plc_text = fetch_placeholder_str(eng_text, ent_mentions)\n",
    "    ent_count = len(ent_mentions)\n",
    "    label_count = {lang: 0 for lang in accepted_langs}\n",
    "    lang_ent_mentions = {key: [] for key in accepted_langs}\n",
    "    for mention in ent_mentions:\n",
    "        entity_id = mention['link']\n",
    "        label_dict = find_entity_labels(entity_id)\n",
    "        # print('label dict: %s' % label_dict)\n",
    "        for key in label_dict:\n",
    "            lang_query = entry['translations'][key]\n",
    "            cur_label = label_dict[key]\n",
    "            if cur_label in lang_query:\n",
    "                # Add lang mention\n",
    "                for match in re.finditer(re.escape(cur_label), lang_query):\n",
    "                    lang_mention = {'link': entity_id,\n",
    "                                    'start': match.start(),\n",
    "                                    'end': match.end(),\n",
    "                                    'placeholder': mention['placeholder']}\n",
    "                    lang_ent_mentions[key].append(lang_mention)\n",
    "                label_count[key] += 1\n",
    "    # print('Extracted language based entity mentions: %s' % lang_ent_mentions)\n",
    "    # check label counts\n",
    "    for key in label_count:\n",
    "        if label_count[key] == ent_count:\n",
    "            lang_query = entry['translations'][key]\n",
    "            # sorting for placeholder logic\n",
    "            lang_ent_mentions[key].sort(key=lambda d: d['start'])\n",
    "            lang_ent_mentions[key]\n",
    "            # Generate placeholder text\n",
    "            lang_plc_text = fetch_placeholder_str(lang_query, lang_ent_mentions[key])\n",
    "            # write to file\n",
    "            # placeholder files\n",
    "            plc_en_file = op_files[key][0]\n",
    "            plc_lang_file = op_files[key][1]\n",
    "            # normal files\n",
    "            norm_en_file = op_files[key][2]\n",
    "            norm_lang_file = op_files[key][3]\n",
    "            # print(eng_plc_text, '\\n', lang_plc_text, '\\n\\n\\n')\n",
    "            plc_en_file.write(eng_plc_text + '\\n')\n",
    "            plc_lang_file.write(lang_plc_text + '\\n')\n",
    "            norm_en_file.write(eng_text + '\\n')\n",
    "            norm_lang_file.write(lang_query + '\\n')\n",
    "    pbar.update(1)\n",
    "print('Processing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "338dedd7-c1a8-4932-ab89-a4c4c3f9e793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f5a78b2-bdcb-4b94-b407-11c09c52b8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files closed.\n"
     ]
    }
   ],
   "source": [
    "# Close the output files\n",
    "for value in op_files.values():\n",
    "    for file_obj in value:\n",
    "        file_obj.close()\n",
    "print('Files closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c19fa-21f6-468c-b475-ae9933266ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to json\n",
    "# This is kept separate from the previous logic to run independently without the need to run previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4500051c-c372-426b-89e5-3e9eb2cb2263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "751aab7f-4249-4ede-a5d4-cbb396da667d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Placeholder] English file(data/it_en_placeholder.txt) lines: 6382\n",
      "[Placeholder] Lang file(data/it_placeholder.txt) lines: 6383\n",
      "\n",
      "[Normal] English file(data/it_en.txt) lines: 6382\n",
      "[Normal] Lang file(data/it.txt) lines: 6383\n",
      "\n",
      "\n",
      "json files created!\n"
     ]
    }
   ],
   "source": [
    "#accepted_langs = ['de', 'pt', 'es', 'fr']\n",
    "accepted_langs = ['it']\n",
    "input_dir = 'data/'\n",
    "output_dir = 'data/json/'\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "for key in accepted_langs:\n",
    "    plc_en_file = '%s%s_en_placeholder.txt' % (input_dir, key)\n",
    "    plc_lang_file = '%s%s_placeholder.txt' % (input_dir, key)\n",
    "    plc_out_file = '%s%s_en_placeholder.json' % (output_dir, key)\n",
    "    with open(plc_en_file, 'r') as en_file_obj,  open(plc_lang_file, 'r') as lang_file_obj, open(plc_out_file, 'w') as out_file_obj:\n",
    "        # Read the files and write json\n",
    "        en_texts = en_file_obj.read().splitlines()\n",
    "        lang_texts = lang_file_obj.read().splitlines()\n",
    "        print('[Placeholder] English file(%s) lines: %d\\n[Placeholder] Lang file(%s) lines: %d\\n' % (plc_en_file, len(en_texts), plc_lang_file, len(lang_texts)))\n",
    "        output_json = []\n",
    "        for en_line, lang_line in zip(en_texts, lang_texts):\n",
    "            output_json.append({'output': en_line, 'input': lang_line})\n",
    "        json.dump(output_json, out_file_obj)\n",
    "\n",
    "    norm_en_file = '%s%s_en.txt' % (input_dir, key)\n",
    "    norm_lang_file = '%s%s.txt' % (input_dir, key)\n",
    "    norm_out_file = '%s%s_en.json' % (output_dir, key)\n",
    "    with open(norm_en_file, 'r') as en_file_obj,  open(norm_lang_file, 'r') as lang_file_obj, open(norm_out_file, 'w') as out_file_obj:\n",
    "        # Read the files and write json\n",
    "        en_texts = en_file_obj.read().splitlines()\n",
    "        lang_texts = lang_file_obj.read().splitlines()\n",
    "        print('[Normal] English file(%s) lines: %d\\n[Normal] Lang file(%s) lines: %d\\n\\n' % (norm_en_file, len(en_texts), norm_lang_file, len(lang_texts)))\n",
    "        output_json = []\n",
    "        for en_line, lang_line in zip(en_texts, lang_texts):\n",
    "            output_json.append({'output': en_line, 'input': lang_line})\n",
    "        json.dump(output_json, out_file_obj)\n",
    "print('json files created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483ce99-0512-4145-a4b9-459168f0ca47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f992a0-6e0e-4bf2-aa99-3c5d109e5850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing load dataset for huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e3a56-3757-4675-a93e-ab01f203465f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1be40-b219-4bc8-9646-eda882154912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/json/de_en*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f121b2d-29a7-492a-9b83-5449225ced9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bdf87-5d42-40b3-99e1-d25770548d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in dataset['train']:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed324a-80be-482d-bdf1-afd00dffe683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "porque",
   "language": "python",
   "name": "porque"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
